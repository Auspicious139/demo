from datetime import datetime, timedelta

# from numpy.typing import _256Bit
from openpyxl import load_workbook
import pandas as pd
import os
import numpy as np
import pandasql
import dmt_util
import time
import yaml
import warnings
import glob
import sqlite3
from sqlalchemy import DateTime
from Log import Log
from Helpers import Helpers

pd.options.mode.chained_assignment = None


class FG_Auto:
    def __init__(
        self,
        month_start="2022-06-01",
        month_end="2022-06-30",
        rep_month=6,
        env="production",
        basename="SubDefault",
        db_path="../../Barclays_Account_Master_DB_v2.db",
        confirmed_workers_including_pay="TRANS_CONFIRMED_WORKERS",
        confirmed_workers_override_file="TRANS_CONFIRMED_WORKERS_OVERRIDE",
        prima="VW_TRANS_PRIMA_LATEST",
        prima_version=0,
        prima_us="../Inputs/PRIMA_US.xlsx",
        prima_uk_ind="../Inputs/PRIMA_UK_IND.xlsx",
        timesheet_on_hold="TRANS_TIMESHEETS_ON_HOLD",
        prima_exclusion_list="TRANS_ACCOUNTS_EXCLUSION_LIST",
        dmt="RAW_DEMAND_MASTER_SRV",
        pir="VW_PIR_HISTORY_FOR_NCE_ANALYSIS",
        timesheet_exclusion_list="TIMESHEET_EXCLUSION_LIST",
        outputs="../Outputs",
        log_level=0,
        log_file=None,
        log_callback=None,
        by=""
    ):

        self.v_month_start = month_start
        self.v_month_end = month_end
        self.v_rep_month = datetime.strptime(month_start, "%Y-%m-%d").month
        self.v_batch_id = basename
        self.v_comm = basename
        self.v_exclude_shift = "N"
        self.v_ms_date = datetime.strptime(month_start, "%Y-%m-%d").date()
        self.v_me_date = datetime.strptime(month_end, "%Y-%m-%d").date()
        self.v_debug_mode = "N"
        self.v_env = env
        self.v_prima_version = prima_version
        self.confirmed_workers_including_pay = confirmed_workers_including_pay
        self.confirmed_workers_override_file = confirmed_workers_override_file
        self.prima = prima
        self.prima_us = prima_us
        self.prima_uk_ind = prima_uk_ind
        self.prima_exclusion_list = prima_exclusion_list
        self.dmt = dmt
        self.pir = pir
        self.timesheet_exclusion_list = timesheet_exclusion_list
        self.timesheets_on_hold = timesheet_on_hold
        self.outputs = outputs
        self.v_log_file = (
            log_file if log_file else f"{outputs}/{basename}_Processing_log.txt"
        )
        self.v_log = open(self.v_log_file, "w")
        self.db_path = db_path
        self.conn = sqlite3.connect(self.db_path)
        self.log = Log(log_file, log_callback)
        self.helpers = Helpers(
            self.log,
            self.v_rep_month,
            self.v_ms_date,
            self.v_me_date,
            self.prima,
            self.timesheet_exclusion_list,
            self.prima_exclusion_list,
            self.timesheets_on_hold,
            by,
            self.conn,
        )

    def log_notice(self, v_line):
        if self.v_env == "production":
            print("+" + "-" * (len(v_line) + 4) + "+")
            print("|  " + v_line + "  |")
            print("+" + "-" * (len(v_line) + 4) + "+")

    def cnt_working_days(self, row: dict) -> int:
        """
        Count number of days in a week that the emp has worked

        Parameters
        ---
        `row`   - current row

        Returns
        ---
        number of days worked in a week
        """
        cnt = 0

        for col, val in row.items():
            if col in [
                "MONDAY_HOURS",
                "TUESDAY_HOURS",
                "WEDNESDAY_HOURS",
                "THURSDAY_HOURS",
                "FRIDAY_HOURS",
                "SATURDAY_HOURS",
                "SUNDAY_HOURS",
            ]:
                if val and not np.isnan(val):
                    cnt += 1

        return cnt

    def final_timesheet_apply_uom(self, v_shift_type):
        # print("here")
        if v_shift_type > 0:
            return "DAY"
        else:
            return ""

    def final_timesheet_apply_ratecat(self, v_shift_type):
        if v_shift_type == 0:
            return "[HOURS WORKED]"
        elif v_shift_type == 1:
            return "First_Shift_HomeBase"
        elif v_shift_type == 2:
            return "Second_Shift_HomeBase"
        elif v_shift_type == 3:
            return "Third_Shift_HomeBase"
        elif v_shift_type == 4:
            return "Third_Shift_HomeBase"

    def apply_final_exclusion(self, row: dict) -> str:
        """
        Parameters
        ---
        `row` - current row

        Returns
        ---
        status of timesheet creation
        """
        # if row['MATCH_PRIORITY'] < 9:
        #     return 'Timesheet Created'

        # bypass all checks if override case
        if row["MATCH_PRIORITY"] == 0:
            return "Timesheet Created"

        if row["PRIMA_DMT_JOIN"] == "left_only":
            return "Could not link IPPF with DMT"

        elif row["SHIFT_TIMESHEET_IND"] == "Y" and self.v_exclude_shift == "Y":
            return "Excluded as Shift Timesheet to be created"

        # elif row["TS_START_DATE"] == datetime.strptime("2000-01-01", "%Y-%m-%d").date():
        #     return "Candidate Start Date as per SR not defined in DMT"

        elif row["IN_MONTH_JOINER_OR_LEAVER"] == "Starts after current reporting month":
            return "Starts after current reporting month"

        elif row["IN_MONTH_JOINER_OR_LEAVER"] == "Left before current reporting month":
            return "Left before current reporting month"

        elif row["PRIMA_DMT_FG_JOIN"] == "left_only":
            return "Could not link DMT and Fieldglass"

        elif (
            row["PRIMA_DMT_FG_JOIN"] == "both"
            and row["BRID"] != row["FG_BRID"]
            and not pd.isna(row["BRID"])
        ):
            return "BRID mismatch between FG and DMT - need to be investigated"

        return "Timesheet Created"

    def apply_ignore_flag(self, row: dict) -> str:
        """
        Parameters
        ---
        `row` - current row

        Returns
        ---
        ignore flags
        """

        try:
            v_rec_date = row["LATES_ASSIGNMENT_REVISION_START_DATE"].date()
            v_rec_date = v_rec_date.strftime("%Y-%m-%d")
        except Exception as e:
            try:
                v_rec_date = row["FIRST_ASSIGNMENT_ORIGINAL_START_DATE"].date()
                v_rec_date = v_rec_date.strftime("%Y-%m-%d")
            except Exception:
                v_rec_date = "1900-01-01"

        if row["MAX_START_DATE"]:
            v_max_date = row["MAX_START_DATE"][:10]
        else:
            v_max_date = None

        if row["MAX_START_DATE_NON_MIGRATED"]:
            v_max_date_nm = row["MAX_START_DATE_NON_MIGRATED"][:10]
        else:
            v_max_date_nm = None

        if row["BRID_DUP_CLASS"] == "ONEVALIDATR" and row["ATR_ID"] == "Migrated":
            return "Another record with valid ATR exists for this BRID hence ignored"

        elif row["BRID_DUP_CLASS"] == "ALLMIGRATED" and v_rec_date != v_max_date:
            return "Another record with latest assignment date exists for this BRID hence ignored"

        elif row["BRID_DUP_CLASS"] == "MULTIVALIDATR" and row["ATR_ID"] == "Migrated":
            return "Another record with valid ATR exists for this BRID hence ignored"

        elif row["BRID_DUP_CLASS"] == "MULTIVALIDATR" and v_rec_date != v_max_date_nm:
            return "Another record with latest assignment date and valid ATR ID exists for this BRID hence ignored"

        elif row["BRID_DUP_CLASS"] == "BOTH_GBP_USD" and row["CURRENCY"] == "GBP":
            return "GBP Record excluded as USD Record exists"

        return "NO"

    def adjust_hours_as_per_loc(
        self,
        v_psl_rate_card: str,
        v_emp_country: str,
        v_hours: float,
        v_ws_date: datetime,
        v_inc: int,
    ) -> float:
        """
        Get the adjusted hours of an emp

        Parameters
        ---
        `v_psl_rate_card`
        `v_emp_country`     - country of the emp
        `v_hours`           - clocked hours by the emp
        `v_ws_date`         - week start date
        `v_inc`             - day code (0 = saturday till 6 = friday)

        Returns
        ---
        *adjusted hours*
        """

        v_curr_date = v_ws_date + timedelta(v_inc)
        v_adj_hours = v_hours

        if not pd.isna(v_psl_rate_card) and v_psl_rate_card.find("Offshore") >= 0:
            v_loc = "IN"
        else:
            v_loc = "NON-IN"

        if v_loc == "IN" and v_hours == 8:
            v_adj_hours = 9

        if v_loc == "IN" and v_hours == 4:
            v_adj_hours = 4.5

        if v_loc == "IN" and v_adj_hours > 9:
            v_adj_hours = 9

        # if v_loc == "NON-IN" and v_adj_hours > 8:
        #     v_adj_hours = 8

        # if ( v_in_mon_jl == 'Y' ) :
        # if ( v_ts_start_date <= v_curr_date <= v_ts_end_date ) :
        # return v_adj_hours
        # else:
        ##
        # print('Making null as TS Start : {}  TS End : {}  and Current : {}'.format(v_ts_start_date,v_ts_end_date,v_curr_date))
        # return  np.NaN
        # else:
        # return v_adj_hours

        return v_adj_hours

    def remove_hours_for_in_month_joiners(
        self,
        v_hours: float,
        v_ts_start_date: datetime,
        v_ts_end_date: datetime,
        v_ws_date: datetime,
        v_inc: int,
        v_in_mon_jl: str,
    ) -> float:
        """
        Get the adjusted hours of an emp if emp is an in-month joiner

        Parameters
        ---
        `v_hours`           - clocked hours by the emp
        `v_ts_start_date`   - start date of the timesheet
        `v_ts_end_date`     - end date of the timesheet
        `v_inc`             - day code (0 = saturday till 6 = friday)

        Returns
        ---
        `adjusted hours` if *in month joiner* else *NaN*
        """

        v_curr_date = v_ws_date + timedelta(v_inc)
        v_adj_hours = v_hours

        if v_in_mon_jl == "Y":
            if v_ts_start_date <= v_curr_date <= v_ts_end_date:
                return v_adj_hours
            else:

                # print('Making null as TS Start : {}  TS End : {}  and Current : {}'.format(v_ts_start_date,v_ts_end_date,v_curr_date))
                return np.NaN

        return v_adj_hours

    def pr_prima_dmt_fg_match(
        self,
        v_df_prima_dmt: pd.DataFrame,
        v_df_fg: pd.DataFrame,
        v_prima_dmt_match_col: str,
        v_fg_match_col: str,
        v_priority: int,
        drop_col: bool = True,
    ):
        """
        Join `PRIMA_DMT` with `FG` based on given set of columns, and assign priority to the join.

        Parameters
        ---
        `v_df_prima_dmt`            - prima + dmt df
        `v_df_fg`                   - fieldglass df
        `v_df_prima_dmt_match_col`  - prima_dmt column used for match
        `v_fg_match_col`            - fieldglass column used for match
        `v_priority`                - priority of the match (can be 0, 1, 2, 3)
        `drop_col`                  - indicate if the `_merge` column plus some others need to be dropped

        Returns
        ---
        merged dataframe based on supplied columns (inner join)
        """

        self.log.log(
            "Priority= {} FG_Col={} DMT_Col={}".format(
                v_priority, v_prima_dmt_match_col, v_fg_match_col
            )
        )

        v_df_prima_dmt = v_df_prima_dmt[v_df_prima_dmt[v_prima_dmt_match_col] != ""]
        v_df_fg = v_df_fg[v_df_fg[v_fg_match_col] != ""]

        # get only common rows so that there is a match (so inner join)
        DF_RET = pd.merge(
            v_df_prima_dmt,
            v_df_fg,
            left_on=v_prima_dmt_match_col,
            right_on=v_fg_match_col,
            how="left",
            suffixes=(None, "_y"),
            indicator=True,
        )

        DF_MATCH: pd.DataFrame = DF_RET[DF_RET["_merge"] == "both"]
        # DF_UNMATCH = DF_RET[DF_RET["_merge"] == "left_only"]

        DF_MATCH["MATCH_PRIORITY"] = v_priority
        # DF_UNMATCH["MATCH_PRIORITY"] = 9

        # if drop_col:
        #     DF_UNMATCH = DF_UNMATCH.drop(
        #         [
        #             "FG_EMAIL",
        #             "FG_BRID",
        #             "FG_ATR_ID",
        #             "JOB_SEEKER_ID",
        #             "COST_CENTRE_CODE",
        #             "FIRST_ASSIGNMENT_ORIGINAL_START_DATE",
        #             "LATES_ASSIGNMENT_REVISION_START_DATE",
        #             "DATA_FROM_OVERRIDE_FILE",
        #             "_merge",
        #         ],
        #         axis=1,
        #     )

        return DF_MATCH

    def did_emp_skipped_week(self, df_prima_all: pd.DataFrame):
        """
        Return the number of weeks employee worked and the total weeks there was in the month

        Parameters
        ---
        `df_prima_all`  -   PRIMA dataframe

        Returns
        ---
        dataframe containing worked weeks, total weeks and if worked last week
        """

        all_weeks = []

        v_date = datetime.strptime(self.v_month_start, "%Y-%m-%d").date()

        for i in range(8):

            v_we_date = (
                v_date - timedelta(v_date.weekday()) + timedelta(4) + timedelta(weeks=i)
            )
            v_ws_date = v_we_date - timedelta(6)
            if (
                v_we_date.month == self.v_rep_month
                or v_ws_date.month == self.v_rep_month
            ):
                all_weeks.append(v_we_date)

        # print(all_weeks)

        DF_EMP_WEEKS = pd.DataFrame(
            columns=["EMP_SERIAL", "WEEKS_CLOCKED", "TOTAL_WEEKS", "LAST_WEEK_CLOCKED"]
        )

        emp_weeks: dict[str, int] = dict()
        emp_weeks_last: dict[str, int] = dict()

        df_prima_all = df_prima_all[["EMP_SERIAL", "WEEKENDING_DATE", "TOTAL_HOURS"]]

        # df_prima_all = df_prima_all.groupby(by=['EMP_SERIAL', 'WEEKENDING_DATE']).sum().reset_index()

        # df_prima_all = df_prima_all.reset_index()

        week_days = [
            "FRIDAY_HOURS",
            "THURSDAY_HOURS",
            "WEDNESDAY_HOURS",
            "TUESDAY_HOURS",
            "MONDAY_HOURS",
            "SUNDAY_HOURS",
            "SATURDAY_HOURS",
        ]
        week_dates = []

        for row in df_prima_all.itertuples():
            we_date = pd.to_datetime(
                getattr(row, "WEEKENDING_DATE"), format="%Y-%m-%d"
            ).date()
            if we_date in all_weeks:

                total_days = 0
                for i in range(7):
                    curr_date = we_date - timedelta(i)

                    if curr_date.month == self.v_rep_month:
                        total_days += 1

                if total_days:
                    try:
                        emp_weeks[getattr(row, "EMP_SERIAL")] += 1
                    except Exception:
                        emp_weeks[getattr(row, "EMP_SERIAL")] = 1

            if we_date == all_weeks[-1]:
                emp_weeks_last[getattr(row, "EMP_SERIAL")] = 1

        total_weeks = len(all_weeks)

        for emp_id, weeks in emp_weeks.items():
            DF_EMP_WEEKS = DF_EMP_WEEKS.append(
                pd.DataFrame(
                    np.array(
                        [
                            [
                                emp_id,
                                weeks,
                                total_weeks,
                                "No" if emp_id not in emp_weeks_last else "Yes",
                            ]
                        ]
                    ),
                    columns=[
                        "EMP_SERIAL",
                        "WEEKS_CLOCKED",
                        "TOTAL_WEEKS",
                        "LAST_WEEK_CLOCKED",
                    ],
                ),
                ignore_index=True,
            )

        return DF_EMP_WEEKS

    def process_prima(self):
        """
        Create timesheets from PRIMA data for accounts clocking in Barclays

        Order of operation
        ---
        1. Take the list of emp from `PRIMA`
        2. Exclude records in `PRIMA`
            - Based on `EMP_ID` using `TIMESHEET_EXCLUSION_LIST`
            - Based on `ACCOUNT_ID` using `PRIMA_EXCLUSION_LIST`
        3. Make `DF_PRIMA_GRP` where hours are summed up each day for each emp in the required format. 
            A row for each week in a month should be present even if the emp joined late.
        4. Read FG
        5. Mark all BRID that appear more than once and apply ignore flags on them and get a dataframe with all unique BRIDs
        6. Read DMT and PIR and process DMT using PIR
        7. Join PRIMA with DMT on `EMP_SERIAL`
        8. Read Confirmed Workers Override file on `EMP_ID`
        9. Establish `FG` link with `PRIMA_DMT` on
            1. `EMAIL_ID`
            2. `BRID`
            3. `ATR_ID`
        10. Assign buckets after hours adjustment

        """

        # print(type(v_ms_date))
        self.log.log("Stage 000 - Month Start Date : {}".format(self.v_ms_date), 2)
        # print(type(v_me_date))
        self.log.log("Stage 000 - Month End Date : {}".format(self.v_me_date), 2)
        self.log.log(
            "Stage 000 - Exclude Shift Timesheets : {}".format(self.v_exclude_shift), 2
        )

        self.log.log(
            "Stage 010 - Reading non billable Accounts to be excluded from IPPF", 2
        )

        DF_PRIMA, DF_PRIMA_ONBR, DF_PRIMA_CH = self.helpers.read_prima()

        try:
            DF_PRIMA["WE_DATE"] = DF_PRIMA["WEEKENDING_DATE"].apply(
                lambda r: datetime.strptime(r, "%Y-%m-%d").date()
            )
            DF_PRIMA["WS_DATE"] = DF_PRIMA["WE_DATE"] - timedelta(6)
        except Exception as e:
            DF_PRIMA["WE_DATE"] = np.NaN
            DF_PRIMA["WS_DATE"] = np.NaN
            self.log.log(str(e))

        self.log.log("Stage 100 - Aggregate Ippf data at Week and EMP level", 2)
        self.log.log(
            "Stage 110 - Keep only those weeks where week start or week end date belongs to current reporting period ",
            2,
        )

        v_sql_text = """
        SELECT WS_DATE, WE_DATE, A.EMP_SERIAL, EMP_COUNTRY, EMP_LASTNAME,
            SUM(TS_HOURS)    AS TS_HOURS,
            TS_DAY,
            SHIFT_TIMESHEET_IND,
            0 as SHIFT_TYPE
        FROM   DF_PRIMA A
        JOIN   ( SELECT EMP_SERIAL, 
                        MAX(CASE WHEN OVERTIME_IND IN ('1','2','3','4') THEN 'Y' ELSE 'N' END) as SHIFT_TIMESHEET_IND
                FROM DF_PRIMA GROUP BY EMP_SERIAL
            ) B
        ON A.EMP_SERIAL = B.EMP_SERIAL       
        WHERE  ( strftime('%m',WS_DATE)+0 = {} OR strftime('%m',WE_DATE)+0 = {} ) 
        GROUP BY WS_DATE, WE_DATE, A.EMP_SERIAL, EMP_COUNTRY, EMP_LASTNAME, TS_DAY, SHIFT_TIMESHEET_IND, SHIFT_TYPE
        """.format(
            self.v_rep_month, self.v_rep_month
        )

        DF_PRIMA_GRP = pandasql.sqldf(v_sql_text, locals())

        DF_PRIMA_GRP["SORT_ORDER"] = 1

        v_sql_text = """
        SELECT WS_DATE, WE_DATE, A.EMP_SERIAL, EMP_COUNTRY, EMP_LASTNAME,
            SUM(TS_HOURS)    AS TS_HOURS,
            TS_DAY,
            'Y' AS SHIFT_TIMESHEET_IND,
            OVERTIME_IND + 0 as SHIFT_TYPE
        FROM   DF_PRIMA A
        WHERE  ( strftime('%m',WS_DATE)+0 = {} OR strftime('%m',WE_DATE)+0 = {} ) AND OVERTIME_IND IN ( '1','2','3','4')
        GROUP BY WS_DATE, WE_DATE, A.EMP_SERIAL, EMP_COUNTRY, EMP_LASTNAME, SHIFT_TYPE, TS_DAY, SHIFT_TIMESHEET_IND 
        """.format(
            self.v_rep_month, self.v_rep_month
        )

        DF_PRIMA_GRP_SHIFT = pandasql.sqldf(v_sql_text, locals())

        DF_PRIMA_GRP_SHIFT["SORT_ORDER"] = 1

        try:
            DF_PRIMA_GRP["WS_DATE"] = DF_PRIMA_GRP["WS_DATE"].apply(
                lambda r: datetime.strptime(r, "%Y-%m-%d").date()
            )
            DF_PRIMA_GRP["WE_DATE"] = DF_PRIMA_GRP["WE_DATE"].apply(
                lambda r: datetime.strptime(r, "%Y-%m-%d").date()
            )
        except Exception as e:
            DF_PRIMA_GRP["WS_DATE"] = np.NaN
            DF_PRIMA_GRP["WE_DATE"] = np.NaN
            self.log_notice(str(e))

        try:
            DF_PRIMA_GRP_SHIFT["WS_DATE"] = DF_PRIMA_GRP_SHIFT["WS_DATE"].apply(
                lambda r: datetime.strptime(r, "%Y-%m-%d").date()
            )
            DF_PRIMA_GRP_SHIFT["WE_DATE"] = DF_PRIMA_GRP_SHIFT["WE_DATE"].apply(
                lambda r: datetime.strptime(r, "%Y-%m-%d").date()
            )
        except Exception as e:
            DF_PRIMA_GRP_SHIFT["WS_DATE"] = np.NaN
            DF_PRIMA_GRP_SHIFT["WE_DATE"] = np.NaN
            self.log_notice(str(e))

        self.log.log(
            "stage 120 - Get Unique list of employees from Prima for whom timessehts to be created in fieldglass"
        )

        DF_PRIMA_EMP_LIST = DF_PRIMA_GRP[
            [
                "EMP_SERIAL",
                "EMP_COUNTRY",
                "EMP_LASTNAME",
                "SHIFT_TIMESHEET_IND",
            ]
        ]
        DF_PRIMA_EMP_LIST = DF_PRIMA_EMP_LIST.drop_duplicates(keep="first")

        self.log.log("stage 130 - Assign Initial buckets")

        DF_PRIMA_EMP_LIST = DF_PRIMA_EMP_LIST.reset_index()
        DF_PRIMA_EMP_LIST["UNIQUE_ID"] = DF_PRIMA_EMP_LIST.index + 1

        DF_PRIMA_EMP_LIST["BUCKET_ID"] = DF_PRIMA_EMP_LIST["UNIQUE_ID"] // 600

        # start buckets from 1
        DF_PRIMA_EMP_LIST["BUCKET_ID"] = DF_PRIMA_EMP_LIST["BUCKET_ID"] + 1

        self.log.log(
            "stage 130 - Expand Prima Data to include all weeks in reporting month in case something is missing"
        )

        DF_WEEK = pd.DataFrame()
        v_date = datetime.strptime(self.v_month_start, "%Y-%m-%d").date()
        i = 0

        # make a datasheet for each week start date with sort priority 9

        # WS_DATE  \  SORT_ORDER
        # ---------\-------------
        # 1        \  9
        # 8        \  9
        # 15       \  9
        # 21       \  9
        while i <= 8:

            v_we_date = (
                v_date - timedelta(v_date.weekday()) + timedelta(4) + timedelta(weeks=i)
            )
            v_ws_date = v_we_date - timedelta(6)
            if (
                v_we_date.month == self.v_rep_month
                or v_ws_date.month == self.v_rep_month
            ):

                DF_WEEK_TEMP = pd.DataFrame(
                    [[v_ws_date, v_we_date, 1, 9]],
                    columns=["WS_DATE", "WE_DATE", "KEY", "SORT_ORDER"],
                )
                DF_WEEK = DF_WEEK.append(DF_WEEK_TEMP)

            i = i + 1

        DF_PRIMA_EMP_LIST["KEY"] = 1

        # merge emp list with the timesheet data to bring in the extra rows needed for the timesheet format
        # mergeing on key will cross join each emp with each timesheet data

        # EMP_CODE  \ WS_DATE  \  SORT_ORDER  \ M \ T \ W \ T \ F \ S \ S
        # ----------\----------\--------------\---\---\---\---\---\---\--
        # 1         \ 1        \  9           \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0
        # 1         \ 8        \  9           \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0
        # 1         \ 15       \  9           \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0
        # 1         \ 21       \  9           \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0
        # ------------------------------------\---\---\---\---\---\---\--
        # 2         \ 1        \  9           \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0
        # 2         \ 8        \  9           \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0
        # 2         \ 15       \  9           \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0
        # 2         \ 21       \  9           \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0
        DF_PRIMA_EMP_LIST_ALL_WEEKS = pd.merge(
            DF_PRIMA_EMP_LIST, DF_WEEK, how="inner", left_on="KEY", right_on="KEY"
        )

        DF_PRIMA_EMP_LIST_ALL_WEEKS["MONDAY_HOURS"] = 0
        DF_PRIMA_EMP_LIST_ALL_WEEKS["TUESDAY_HOURS"] = 0
        DF_PRIMA_EMP_LIST_ALL_WEEKS["WEDNESDAY_HOURS"] = 0
        DF_PRIMA_EMP_LIST_ALL_WEEKS["THURSDAY_HOURS"] = 0
        DF_PRIMA_EMP_LIST_ALL_WEEKS["FRIDAY_HOURS"] = 0
        DF_PRIMA_EMP_LIST_ALL_WEEKS["SATURDAY_HOURS"] = 0
        DF_PRIMA_EMP_LIST_ALL_WEEKS["SUNDAY_HOURS"] = 0

        DF_PRIMA_EMP_LIST_ALL_WEEKS = DF_PRIMA_EMP_LIST_ALL_WEEKS[
            [
                "WS_DATE",
                "WE_DATE",
                "EMP_SERIAL",
                "EMP_COUNTRY",
                "EMP_LASTNAME",
                "SATURDAY_HOURS",
                "SUNDAY_HOURS",
                "MONDAY_HOURS",
                "TUESDAY_HOURS",
                "WEDNESDAY_HOURS",
                "THURSDAY_HOURS",
                "FRIDAY_HOURS",
                "SORT_ORDER",
            ]
        ]

        # pivot the PRIMA_GRP table
        DF_PRIMA_GRP = pd.pivot_table(
            DF_PRIMA_GRP,
            columns=["TS_DAY"],
            values="TS_HOURS",
            index=[
                "WS_DATE",
                "WE_DATE",
                "EMP_SERIAL",
                "EMP_COUNTRY",
                "EMP_LASTNAME",
                "SHIFT_TIMESHEET_IND",
                "SHIFT_TYPE",
                "SORT_ORDER",
            ],
        )

        DF_PRIMA_GRP = DF_PRIMA_GRP.reset_index()

        DF_PRIMA_GRP = DF_PRIMA_GRP.rename(
            columns={
                "MONDAY": "MONDAY_HOURS",
                "TUESDAY": "TUESDAY_HOURS",
                "WEDNESDAY": "WEDNESDAY_HOURS",
                "THURSDAY": "THURSDAY_HOURS",
                "FRIDAY": "FRIDAY_HOURS",
                "SATURDAY": "SATURDAY_HOURS",
                "SUNDAY": "SUNDAY_HOURS",
            }
        )

        # pivot the DF_PRIMA_GRP_SHIFT table
        DF_PRIMA_GRP_SHIFT = pd.pivot_table(
            DF_PRIMA_GRP_SHIFT,
            columns=["TS_DAY"],
            values="TS_HOURS",
            index=[
                "WS_DATE",
                "WE_DATE",
                "EMP_SERIAL",
                "EMP_COUNTRY",
                "EMP_LASTNAME",
                "SHIFT_TIMESHEET_IND",
                "SHIFT_TYPE",
                "SORT_ORDER",
            ],
        )

        DF_PRIMA_GRP_SHIFT = DF_PRIMA_GRP_SHIFT.reset_index()

        DF_PRIMA_GRP_SHIFT = DF_PRIMA_GRP_SHIFT.rename(
            columns={
                "MONDAY": "MONDAY_HOURS",
                "TUESDAY": "TUESDAY_HOURS",
                "WEDNESDAY": "WEDNESDAY_HOURS",
                "THURSDAY": "THURSDAY_HOURS",
                "FRIDAY": "FRIDAY_HOURS",
                "SATURDAY": "SATURDAY_HOURS",
                "SUNDAY": "SUNDAY_HOURS",
            }
        )

        # append the created timesheet with the existing data
        # the existing data from PRIMA will have sort order of 1 and weekdays data in some cells will not be empty
        # eg: the emp has joined on 8 (so he has missing data for 1)

        # EMP_CODE  \ WS_DATE  \  SORT_ORDER  \ M \ T \ W \ T \ F \ S \ S
        # ----------\----------\--------------\---\---\---\---\---\---\--
        # 1         \ 1        \  9           \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0
        # 1         \ 8        \  9           \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0
        # 1         \ 8        \  1           \ 9 \ 9 \ 8 \ 9 \ 9 \ 0 \ 0
        # 1         \ 15       \  9           \ 9 \ 9 \ 8 \ 9 \ 9 \ 0 \ 0
        # 1         \ 21       \  9           \ 9 \ 9 \ 8 \ 9 \ 9 \ 0 \ 0
        DF_PRIMA_GRP = DF_PRIMA_GRP.append(DF_PRIMA_EMP_LIST_ALL_WEEKS)
        DF_PRIMA_GRP.sort_values(
            ["WS_DATE", "EMP_SERIAL", "SORT_ORDER"],
            axis=0,
            ascending=True,
            inplace=True,
            na_position="last",
        )

        # EMP_CODE  \ WS_DATE  \  SORT_ORDER  \ M \ T \ W \ T \ F \ S \ S
        # ----------\----------\--------------\---\---\---\---\---\---\--
        # 1         \ 1        \  9           \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0
        # 1         \ 8        \  1           \ 9 \ 9 \ 8 \ 9 \ 9 \ 0 \ 0
        # 1         \ 15       \  9           \ 9 \ 9 \ 8 \ 9 \ 9 \ 0 \ 0
        # 1         \ 21       \  9           \ 9 \ 9 \ 8 \ 9 \ 9 \ 0 \ 0
        DF_PRIMA_GRP = DF_PRIMA_GRP.drop_duplicates(
            subset=["WS_DATE", "EMP_SERIAL"], keep="first"
        )

        if DF_PRIMA_GRP_SHIFT.shape[0] > 0:
            DF_PRIMA_GRP = DF_PRIMA_GRP.append(DF_PRIMA_GRP_SHIFT)

        self.log.log(
            "stage 140 - PRIMA Data expanded to include missing weeks - now we have data for complete month"
        )

        self.log.log(
            "stage 150 - Set non month days to NULL. This is applicable for weeks who falls across months."
        )

        try:
            DF_PRIMA_GRP["SATURDAY_HOURS"] = DF_PRIMA_GRP.apply(
                lambda x: self.helpers.trim_non_month_days(
                    x["WS_DATE"], 0, x["SATURDAY_HOURS"]
                ),
                axis=1,
            )
            DF_PRIMA_GRP["SUNDAY_HOURS"] = DF_PRIMA_GRP.apply(
                lambda x: self.helpers.trim_non_month_days(
                    x["WS_DATE"], 1, x["SUNDAY_HOURS"]
                ),
                axis=1,
            )
            DF_PRIMA_GRP["MONDAY_HOURS"] = DF_PRIMA_GRP.apply(
                lambda x: self.helpers.trim_non_month_days(
                    x["WS_DATE"], 2, x["MONDAY_HOURS"]
                ),
                axis=1,
            )
            DF_PRIMA_GRP["TUESDAY_HOURS"] = DF_PRIMA_GRP.apply(
                lambda x: self.helpers.trim_non_month_days(
                    x["WS_DATE"], 3, x["TUESDAY_HOURS"]
                ),
                axis=1,
            )
            DF_PRIMA_GRP["WEDNESDAY_HOURS"] = DF_PRIMA_GRP.apply(
                lambda x: self.helpers.trim_non_month_days(
                    x["WS_DATE"], 4, x["WEDNESDAY_HOURS"]
                ),
                axis=1,
            )
            DF_PRIMA_GRP["THURSDAY_HOURS"] = DF_PRIMA_GRP.apply(
                lambda x: self.helpers.trim_non_month_days(
                    x["WS_DATE"], 5, x["THURSDAY_HOURS"]
                ),
                axis=1,
            )
            DF_PRIMA_GRP["FRIDAY_HOURS"] = DF_PRIMA_GRP.apply(
                lambda x: self.helpers.trim_non_month_days(
                    x["WS_DATE"], 6, x["FRIDAY_HOURS"]
                ),
                axis=1,
            )

            DF_PRIMA_GRP["PRIMA_DAYS"] = DF_PRIMA_GRP.apply(
                lambda x: self.cnt_working_days(x), axis=1
            )
        except Exception as e:
            DF_PRIMA_GRP["PRIMA_DAYS"] = np.NaN
            self.log_notice(str(e))

        self.log.log("stage 160 - Processing Confirmed_Workers_including_Pay", 2)

        query = f"""SELECT * FROM {self.confirmed_workers_including_pay}"""
        # file_df = pd.ExcelFile(self.confirmed_workers_including_pay)
        # DF_CW = file_df.parse("report", skiprows=1)
        DF_CW = pd.read_sql(
            query,
            self.conn,
            parse_dates=[
                "FIRST_ASSIGNMENT_ORIGINAL_START_DATE",
                "LATES_ASSIGNMENT_REVISION_START_DATE",
            ],
        )

        DF_CW.columns = DF_CW.columns.str.replace(" ", "_")
        DF_CW.columns = DF_CW.columns.str.replace("(", "_")
        DF_CW.columns = DF_CW.columns.str.replace(")", "_")
        DF_CW.columns = DF_CW.columns.str.replace("/", "_")
        DF_CW.columns = DF_CW.columns.str.replace("__", "_")
        DF_CW.columns = DF_CW.columns.str.replace("__", "_")
        DF_CW.columns = DF_CW.columns.str.replace("__", "_")
        DF_CW.columns = DF_CW.columns.str.upper()

        DF_CW = DF_CW[
            [
                "ATR_ID",
                "BRID",
                "JOB_SEEKER_ID",
                "CURRENCY",
                "COST_CENTRE_CODE",
                "FIRST_ASSIGNMENT_ORIGINAL_START_DATE",
                "LATES_ASSIGNMENT_REVISION_START_DATE",
                "ASSIGNMENT_STATUS",
                "JOB_SEEKER_EMAIL",
            ]
        ]

        # remove duplicates on whole columns
        DF_CW = DF_CW.drop_duplicates()

        try:
            DF_CW["ATR_ID"] = DF_CW.apply(
                lambda x: self.helpers.cleanup_atr_id(x["ATR_ID"]), axis=1
            )
        except Exception as e:
            self.log_notice(str(e))

        self.log.log(
            "Sateg 170 - Confirmed workes record count : {}".format(DF_CW.shape[0])
        )

        self.log.log(
            "Stage 180 - Remove records with Empty BRID from Confirmed_Workers_including_Pay"
        )

        DF_CW_NOBRID = DF_CW[DF_CW["BRID"] == ""]

        # replace all empty BRID with dummy_{index_of_the_row}
        try:
            DF_CW["BRID"] = DF_CW.apply(
                lambda x: f"dummy_{x.name}" if not x["BRID"] else x["BRID"], axis=1,
            )
        except Exception as e:
            self.log_notice(str(e))

        DF_CW = DF_CW[DF_CW["BRID"] != ""]
        DF_CW["BRID"] = DF_CW["BRID"].str.replace("  ", "")
        DF_CW["BRID"] = DF_CW["BRID"].str.replace("  ", "")
        DF_CW["BRID"] = DF_CW["BRID"].str.replace(" ", "")
        DF_CW["BRID"] = DF_CW["BRID"].str.strip()

        self.log.log(
            "Satge 190 - Count of Confirmed workes post empty BRID record removal : {}".format(
                DF_CW.shape[0]
            )
        )

        self.log.log(
            "Stage 200 - Handling cases where same BRID exists on multiple rows in Confirmed_Workers_including_Pay",
            2,
        )

        # mark all duplicates as False
        # this df contains duplicate values (if a BRID appear two times, both of them will end up here since keep=False)
        # taking not of this df ~DF_CW_DUP will only contain data that appears exactly once on BRID
        DF_CW_DUP = DF_CW[DF_CW.duplicated("BRID", False)]
        DF_CW_DUP = DF_CW_DUP.sort_values(by="BRID")

        # duplicates cannot be ignored as they can have different Job_seeker IDs. If there is a duplicate the following is done to select which one to keep.
        # fill the column with LATES_ASSIGNMENT_REVISION_START_DATE if it is not null
        # else fill it with FIRST_ASSIGNMENT_REVISION_START_DATE
        DF_CW_DUP["LATES_ASSIGNMENT_REVISION_START_DATE"] = np.where(
            DF_CW_DUP["LATES_ASSIGNMENT_REVISION_START_DATE"].isnull(),
            DF_CW_DUP["FIRST_ASSIGNMENT_ORIGINAL_START_DATE"],
            DF_CW_DUP["LATES_ASSIGNMENT_REVISION_START_DATE"],
        )

        DF_CW_DUP["LATES_ASSIGNMENT_REVISION_START_DATE"] = DF_CW_DUP[
            "LATES_ASSIGNMENT_REVISION_START_DATE"
        ].apply(pd.Timestamp)
        DF_CW_DUP["LATES_ASSIGNMENT_REVISION_START_DATE"] = pd.to_datetime(
            DF_CW_DUP["LATES_ASSIGNMENT_REVISION_START_DATE"], format="%Y-%m-%d"
        )

        ### Logic used in derivation of BRID_DUP_CLASS  - to be checked in same order         ###
        # BOTH_GBP_USD  - If one or more record has GBP and one or more record has USD currency #
        # ALLMIGRATED   - If ALL ATR IDs are set to Migrated                                    #
        # ONEVALIDATR   - If one or more ATR IDs are set to Migrated and one valid ATR exists   #
        # MULTIVALIDATR - If more than one valid ATR IDs exists                                 #

        v_sql_text = """
        SELECT BRID,
            CASE WHEN CURR_GBP_CNT >= 1 and CURR_USD_CNT >= 1 THEN 'BOTH_GBP_USD'
                    WHEN BRID_REC_CNT = MIGRATED_ATR_REC_CNT THEN 'ALLMIGRATED' 
                    WHEN VALID_ATR_ID_REC_CNT = 1 THEN 'ONEVALIDATR'
                    WHEN VALID_ATR_ID_REC_CNT > 1 THEN 'MULTIVALIDATR'
            END as BRID_DUP_CLASS,
            MAX_START_DATE,
            MAX_START_DATE_NON_MIGRATED       
        FROM     
        (
        SELECT BRID,
            COUNT(1) as BRID_REC_CNT,
            SUM(CASE WHEN CURRENCY = 'GBP' then 1 ELSE 0 END ) as CURR_GBP_CNT,
            SUM(CASE WHEN CURRENCY = 'USD' then 1 ELSE 0 END ) as CURR_USD_CNT,
            SUM(CASE WHEN ATR_ID = 'Migrated' THEN 0 ELSE 1 END) as VALID_ATR_ID_REC_CNT,
            SUM(CASE WHEN ATR_ID = 'Migrated' THEN 1 ELSE 0 END) as MIGRATED_ATR_REC_CNT,
            MAX(CASE WHEN ASSIGNMENT_STATUS = 'Confirmed' THEN LATES_ASSIGNMENT_REVISION_START_DATE END) as MAX_START_DATE,
            MAX(CASE WHEN ASSIGNMENT_STATUS = 'Confirmed' AND ATR_ID <> 'Migrated' THEN LATES_ASSIGNMENT_REVISION_START_DATE END) as MAX_START_DATE_NON_MIGRATED
        FROM   DF_CW_DUP
        GROUP BY BRID
        ) a
        """

        DF_CW_DUP_CLASS = pandasql.sqldf(v_sql_text, locals())

        DF_CW_DUP = pd.merge(
            DF_CW_DUP, DF_CW_DUP_CLASS, how="inner", left_on="BRID", right_on="BRID"
        )

        DF_CW_DUP["IGNORE_IND"] = ""

        try:
            DF_CW_DUP["IGNORE_IND"] = DF_CW_DUP.apply(self.apply_ignore_flag, axis=1)
        except Exception as e:
            DF_CW_DUP["IGNORE_IND"] = np.NaN
            self.log_notice(str(e))

        DF_CW_RES_DUP = DF_CW_DUP[DF_CW_DUP["IGNORE_IND"] == "NO"]

        DF_CW_RES_STILLDUP = DF_CW_RES_DUP[DF_CW_RES_DUP.duplicated("BRID", False)]

        self.log.log(
            "Stage 210 - Resolve BRID counts pre dup removal: {}".format(
                DF_CW_RES_DUP.shape[0]
            )
        )

        DF_CW_RES_DUP = DF_CW_RES_DUP.drop_duplicates(subset=["BRID"], keep=False)

        self.log.log(
            "Stage 220 - Resolve BRID counts post dup removal : {}".format(
                DF_CW_RES_DUP.shape[0]
            )
        )

        DF_CW_RES_DUP = DF_CW_RES_DUP.drop(
            [
                "BRID_DUP_CLASS",
                "MAX_START_DATE",
                "MAX_START_DATE_NON_MIGRATED",
                "IGNORE_IND",
            ],
            axis=1,
        )

        self.log.log(
            "Stage 230 - Merging resolved data back in Confirmed_Workers_including_Pay"
        )

        DF_CW_NEW = DF_CW.drop_duplicates(subset=["BRID"], keep=False)

        # all unique records that has count of 1 + keeping only 1 from duplicates
        DF_CW_NEW = DF_CW_NEW.append(DF_CW_RES_DUP)

        self.log.log("Stage 220 - Attempt 2 of resolving data")
        # DF_CW_RES_STILLDUP = DF_CW_RES_DUP[DF_CW_RES_DUP.duplicated('BRID',False)]
        DF_CW_RES_STILLDUP.sort_values(
            ["BRID", "JOB_SEEKER_ID"],
            axis=0,
            ascending=True,
            inplace=True,
            na_position="last",
        )
        DF_CW_RES_STILLDUP = DF_CW_RES_STILLDUP.drop_duplicates(
            subset=["BRID"], keep="last"
        )

        # all unique records that has count of 1 + keeping only 1 from remaining duplicates
        DF_CW_NEW = DF_CW_NEW.append(DF_CW_RES_STILLDUP)

        self.log.log("Stage 230 : Load DMT Data", 2)

        query = f"""SELECT * FROM {self.dmt}"""
        # file_df = pd.ExcelFile(self.dmt)
        # DF_DMT = file_df.parse("Demand Master", skiprows=4, nrows=10000)
        DF_DMT = pd.read_sql(
            query,
            self.conn,
            parse_dates=[
                "IBM_RESPONDED_TO_SR_ON_DATE",
                "CANDIDATE_START_DATE",
                "CANDIDATE_END_DATE",
                "BRID_AVAILABLE_DATE",
                "RSA_TOKEN_AVAILABLE_DATE",
            ],
        )

        DF_DMT = DF_DMT[DF_DMT["TRANSACTION_CYCLE"] != ""]
        DF_DMT = DF_DMT[DF_DMT["TRANSACTION_CYCLE"] != "x"]

        DF_DMT["BRID"] = DF_DMT["BRID"].apply(lambda x: x if x != "0" else np.NaN)

        self.log.log("Stage 240 : Load PIR Data", 2)

        query = f"""SELECT * FROM {self.pir}"""
        # file_df = pd.ExcelFile(self.pir)
        # DF_PIR = file_df.parse()
        DF_PIR = pd.read_sql(query, self.conn)

        self.log.log("Stage 250 : Enrich DMT Data with PIR")

        DF_DMT.rename(
            index=str,
            columns={
                "DMT_SRNO": "SR_NO",
                "TRANSACTION_CYCLE": "TC",
                "DEMAND_PROBABILITY": "DP",
                "SUPPLY_STATUS": "SS",
                "IBM_RESPONDED_TO_SR_ON_DATE": "SR_RESPONSE_DATE",
                "CANDIDATE_START_DATE": "CAN_SR_START_DATE",
                "EMP_ID": "DMT_EMP_CODE",
                "IBM_EMAIL_ID": "DMT_EMAIL",
                "CANDIDATE_END_DATE": "CAN_END_DATE",
                "BRID_AVAILABLE_DATE": "DATE_BRID_AVAILABLE",
                "RSA_TOKEN_AVAILABLE_DATE": "DATE_RSA_TOKEN_AVAILABLE",
                "BARCLAYS_SR_NO": "DMT_ATR_ID",
            },
            inplace=True,
        )
        try:
            DF_DMT = DF_DMT[
                [
                    "SR_NO",
                    "TC",
                    "DP",
                    "SS",
                    "SR_RESPONSE_DATE",
                    "CAN_SR_START_DATE",
                    "DATE_BRID_AVAILABLE",
                    "DATE_RSA_TOKEN_AVAILABLE",
                    "CAN_END_DATE",
                    "PSL_RATE_CARD",
                    "DMT_EMP_CODE",
                    "DMT_EMAIL",
                    "BRID",
                    "DMT_ATR_ID",
                ]
            ]
        except Exception as e:
            DF_DMT["CAN_SR_START_DATE"] = pd.NaT
            DF_DMT["DATE_BRID_AVAILABLE"] = pd.NaT
            DF_DMT["SR_RESPONSE_DATE"] = pd.NaT
            DF_DMT = DF_DMT[
                [
                    "SR_NO",
                    "TC",
                    "DP",
                    "SS",
                    "SR_RESPONSE_DATE",
                    "CAN_SR_START_DATE",
                    "DATE_BRID_AVAILABLE",
                    "DATE_RSA_TOKEN_AVAILABLE",
                    "CAN_END_DATE",
                    "PSL_RATE_CARD",
                    "DMT_EMP_CODE",
                    "DMT_EMAIL",
                    "BRID",
                    "DMT_ATR_ID",
                ]
            ]

        DF_DMT["PSL_RATE_CARD"].fillna("Offshore", inplace=True)

        DF_DMT = DF_DMT.replace(to_replace=[None], value=np.NaN)

        try:
            DF_DMT["DMT_EMP_CODE"] = DF_DMT.apply(
                lambda x: self.helpers.cleanup_dmt_emp_code(x["DMT_EMP_CODE"]), axis=1
            )
            DF_DMT["DMT_ATR_ID"] = DF_DMT.apply(
                lambda x: self.helpers.cleanup_atr_id(x["DMT_ATR_ID"]), axis=1
            )
        except Exception as e:
            self.log_notice(str(e))

        self.log.log("Stage 260 - DMT Record count {}".format(DF_DMT.shape[0]))

        DF_DMT = DF_DMT[DF_DMT["DMT_EMP_CODE"] != ""]

        DF_DMT = DF_DMT[
            [
                "SR_NO",
                "TC",
                "DP",
                "SS",
                "PSL_RATE_CARD",
                "SR_RESPONSE_DATE",
                "CAN_SR_START_DATE",
                "DATE_BRID_AVAILABLE",
                "DATE_RSA_TOKEN_AVAILABLE",
                "CAN_END_DATE",
                "DMT_EMP_CODE",
                "DMT_EMAIL",
                "BRID",
                "DMT_ATR_ID",
            ]
        ]

        DF_DMT["BRID"] = DF_DMT["BRID"].str.replace("  ", "")
        DF_DMT["BRID"] = DF_DMT["BRID"].str.replace("  ", "")
        DF_DMT["BRID"] = DF_DMT["BRID"].str.replace(" ", "")
        DF_DMT["BRID"] = DF_DMT["BRID"].str.strip()

        DF_DMT["BRID"] = DF_DMT["BRID"].str.replace(" ", "")
        DF_DMT["BRID"] = DF_DMT["BRID"].str.replace("O", "0")

        #####DF_DMT['IN_MONTH_JOINER_OR_LEAVER']=DF_DMT.apply(lambda x:get_joiner_leaver_in_curr_month(x['TS_START_DATE'],x['TS_END_DATE']),axis=1)

        # print('DMT count : {}'.format(DF_DMT.shape[0]))

        DF_DMT["DMT_EMP_CODE"] = DF_DMT["DMT_EMP_CODE"].astype(str)
        DF_PRIMA_EMP_LIST["EMP_SERIAL"] = DF_PRIMA_EMP_LIST["EMP_SERIAL"].astype(str)

        self.log.log("Stage 290 - Join IPPF and DMT Data at employee level", 2)

        DF_PRIMA_DMT_EMP = pd.merge(
            DF_PRIMA_EMP_LIST,
            DF_DMT,
            how="left",
            left_on="EMP_SERIAL",
            right_on="DMT_EMP_CODE",
            indicator=True,
        )

        DF_PRIMA_DMT_EMP.rename(
            index=str, columns={"_merge": "PRIMA_DMT_JOIN"}, inplace=True
        )

        DF_CW_NEW_1 = DF_CW_NEW[
            [
                "JOB_SEEKER_EMAIL",
                "BRID",
                "ATR_ID",
                "JOB_SEEKER_ID",
                "COST_CENTRE_CODE",
                "FIRST_ASSIGNMENT_ORIGINAL_START_DATE",
                "LATES_ASSIGNMENT_REVISION_START_DATE",
            ]
        ]
        DF_CW_NEW_1["JOB_SEEKER_EMAIL"] = DF_CW_NEW_1["JOB_SEEKER_EMAIL"].str.lower()
        DF_CW_NEW_1.rename(
            index=str,
            columns={
                "JOB_SEEKER_EMAIL": "FG_EMAIL",
                "BRID": "FG_BRID",
                "ATR_ID": "FG_ATR_ID",
            },
            inplace=True,
        )

        DF_CW_NEW_1["DATA_FROM_OVERRIDE_FILE"] = "N"

        DF_CW_NEW_2 = DF_CW_NEW_1
        DF_CW_NEW_3 = DF_CW_NEW_1

        DF_CW_NEW_3 = DF_CW_NEW_3[DF_CW_NEW_3["FG_ATR_ID"] != "Migrated"]

        DF_CW_NEW_1 = DF_CW_NEW_1.drop_duplicates(subset=["FG_EMAIL"], keep=False)
        DF_CW_NEW_2 = DF_CW_NEW_2.drop_duplicates(subset=["FG_BRID"], keep=False)
        DF_CW_NEW_3 = DF_CW_NEW_3.drop_duplicates(subset=["FG_ATR_ID"], keep=False)

        self.log.log("Stage 300 - LOAD Confirmed worker override file", 2)

        query = f"""SELECT * FROM {self.confirmed_workers_override_file}"""
        # file_df = pd.ExcelFile(self.confirmed_workers_override_file)
        # DF_CWOR = file_df.parse("Sheet1")
        DF_CWOR = pd.read_sql(query, self.conn)

        self.log.log(
            "Stage 310 - Confirmed worker override file count : {}".format(
                DF_CWOR.shape[0]
            )
        )

        DF_CWOR.columns = DF_CWOR.columns.str.replace(" ", "_")
        DF_CWOR.columns = DF_CWOR.columns.str.upper()

        DF_CWOR = DF_CWOR.drop_duplicates(subset=["EMP_SERIAL"], keep="first")
        DF_CWOR["FG_EMAIL"] = ""
        DF_CWOR["FIRST_ASSIGNMENT_ORIGINAL_START_DATE"] = None
        DF_CWOR["LATES_ASSIGNMENT_REVISION_START_DATE"] = None
        DF_CWOR["FG_ATR_ID"] = None
        DF_CWOR["DATA_FROM_OVERRIDE_FILE"] = "Y"

        DF_CWOR = DF_CWOR[
            [
                "EMP_SERIAL",
                "FG_EMAIL",
                "FG_BRID",
                "FG_ATR_ID",
                "JOB_SEEKER_ID",
                "COST_CENTRE_CODE",
                "FIRST_ASSIGNMENT_ORIGINAL_START_DATE",
                "LATES_ASSIGNMENT_REVISION_START_DATE",
                "DATA_FROM_OVERRIDE_FILE",
            ]
        ]

        self.log.log("Stage 320 - Joining IPPF_DMT with FG based on override file", 2)

        # mapping FG to PRIMA_DMT:
        # 1. link to override file using emp_id as the override file is created to map these directly
        # 2. Carryover the remaining for match using email
        # 3. Carryover the left for BRID matching
        # 4. Last, try matching using ATR

        # create timesheet for all entries in override file
        prima_dmt_fg_match_0 = self.pr_prima_dmt_fg_match(
            DF_PRIMA_EMP_LIST, DF_CWOR, "EMP_SERIAL", "EMP_SERIAL", 0
        )

        print("=================================")
        print(DF_CWOR)
        print(prima_dmt_fg_match_0)

        # for the rest remove the override entries from DF_PRIMA_DMT_EMP on emp_id
        prima_dmt_fg_match_1 = self.pr_prima_dmt_fg_match(
            DF_PRIMA_DMT_EMP, DF_CW_NEW_1, "DMT_EMAIL", "FG_EMAIL", 1
        )
        prima_dmt_fg_match_2 = self.pr_prima_dmt_fg_match(
            DF_PRIMA_DMT_EMP, DF_CW_NEW_2, "BRID", "FG_BRID", 2
        )
        prima_dmt_fg_match_3 = self.pr_prima_dmt_fg_match(
            DF_PRIMA_DMT_EMP, DF_CW_NEW_3, "DMT_ATR_ID", "FG_ATR_ID", 3
        )

        # make a copy of each of the employee  with match priority 9
        prima_dmt_fg_match_4 = self.pr_prima_dmt_fg_match(
            DF_PRIMA_DMT_EMP, DF_PRIMA_DMT_EMP, "EMP_SERIAL", "EMP_SERIAL", 9, False
        )

        # make the _merge force left_only for the last function call where both df are same
        # here we just want the list of employees
        prima_dmt_fg_match_4["_merge"] = "left_only"

        DF_PRIMA_DMT_FG_EMP = pd.DataFrame()
        DF_PRIMA_DMT_FG_EMP = DF_PRIMA_DMT_FG_EMP.append(prima_dmt_fg_match_0)
        DF_PRIMA_DMT_FG_EMP = DF_PRIMA_DMT_FG_EMP.append(prima_dmt_fg_match_1)
        DF_PRIMA_DMT_FG_EMP = DF_PRIMA_DMT_FG_EMP.append(prima_dmt_fg_match_2)
        DF_PRIMA_DMT_FG_EMP = DF_PRIMA_DMT_FG_EMP.append(prima_dmt_fg_match_3)
        DF_PRIMA_DMT_FG_EMP = DF_PRIMA_DMT_FG_EMP.append(prima_dmt_fg_match_4)

        DF_PRIMA_DMT_FG_EMP.rename(
            index=str, columns={"_merge": "PRIMA_DMT_FG_JOIN"}, inplace=True
        )

        DF_PRIMA_DMT_FG_EMP.sort_values(
            ["EMP_SERIAL", "MATCH_PRIORITY"],
            axis=0,
            ascending=True,
            inplace=True,
            na_position="last",
        )

        DF_PRIMA_DMT_FG_EMP = DF_PRIMA_DMT_FG_EMP.drop_duplicates(subset=["EMP_SERIAL"], keep="first")

        try:
            DF_PRIMA_DMT_FG_EMP["TS_START_DATE"] = DF_PRIMA_DMT_FG_EMP.apply(
                lambda x: self.helpers.get_can_start_date(
                    x["SR_RESPONSE_DATE"],
                    x["CAN_SR_START_DATE"],
                    x["DATE_BRID_AVAILABLE"],
                    x["DATE_RSA_TOKEN_AVAILABLE"],
                    x["FIRST_ASSIGNMENT_ORIGINAL_START_DATE"],
                ),
                axis=1,
            )
            DF_PRIMA_DMT_FG_EMP["TS_END_DATE"] = DF_PRIMA_DMT_FG_EMP.apply(
                lambda x: self.helpers.get_can_end_date(x["CAN_END_DATE"]), axis=1
            )
        except Exception as e:
            DF_PRIMA_DMT_FG_EMP["TS_START_DATE"] = pd.NaT
            DF_PRIMA_DMT_FG_EMP["TS_END_DATE"] = pd.NaT
            self.log_notice(str(e))

        try:
            DF_PRIMA_DMT_FG_EMP[
                "IN_MONTH_JOINER_OR_LEAVER"
            ] = DF_PRIMA_DMT_FG_EMP.apply(
                lambda x: self.helpers.get_joiner_leaver_in_curr_month(
                    x["TS_START_DATE"], x["TS_END_DATE"]
                ),
                axis=1,
            )
        except Exception as e:
            DF_PRIMA_DMT_FG_EMP["IN_MONTH_JOINER_OR_LEAVER"] = np.NaN
            self.log_notice(str(e))

        self.log.log("Stage 350 - Applying Final Exclusion Reason")

        try:
            DF_PRIMA_DMT_FG_EMP["FINAL_EXCLUSION"] = DF_PRIMA_DMT_FG_EMP.apply(
                self.apply_final_exclusion, axis=1
            )
        except Exception as e:
            DF_PRIMA_DMT_FG_EMP["FINAL_EXCLUSION"] = np.NaN
            self.log_notice(str(e))

        # DF_PRIMA_DMT_FG_EMP = pd.merge(DF_PRIMA_DMT_FG_EMP, DF_PRIMA_SKIP, how='left', left_on='EMP_SERIAL', right_on='EMP_SERIAL')

        DF_PRIMA_DMT_FG_EMP_INC = DF_PRIMA_DMT_FG_EMP[
            DF_PRIMA_DMT_FG_EMP["FINAL_EXCLUSION"] == "Timesheet Created"
        ]

        v_sql_text = """SELECT FINAL_EXCLUSION,count(1) as Rec_Count FROM DF_PRIMA_DMT_FG_EMP GROUP BY FINAL_EXCLUSION"""
        DF_SUMMARY = pandasql.sqldf(v_sql_text, locals())

        self.log.log("Stage 360 - Move same month joiners to different bucket 9")

        DF_PRIMA_DMT_FG_EMP_INC.loc[
            DF_PRIMA_DMT_FG_EMP_INC["IN_MONTH_JOINER_OR_LEAVER"] == "Y", "BUCKET_ID"
        ] = 9
        DF_PRIMA_DMT_FG_EMP.loc[
            DF_PRIMA_DMT_FG_EMP["IN_MONTH_JOINER_OR_LEAVER"] == "Y", "BUCKET_ID"
        ] = 9

        self.log.log("Stage 370 - Move Data coming from override file to Bucket 10")

        DF_PRIMA_DMT_FG_EMP_INC.loc[
            DF_PRIMA_DMT_FG_EMP_INC["DATA_FROM_OVERRIDE_FILE"] == "Y", "BUCKET_ID"
        ] = 10
        DF_PRIMA_DMT_FG_EMP.loc[
            DF_PRIMA_DMT_FG_EMP["DATA_FROM_OVERRIDE_FILE"] == "Y", "BUCKET_ID"
        ] = 10

        self.log.log("Stage 375 - Move Shift timesheets to Bucket 11")

        DF_PRIMA_DMT_FG_EMP_INC.loc[
            DF_PRIMA_DMT_FG_EMP_INC["SHIFT_TIMESHEET_IND"] == "Y", "BUCKET_ID"
        ] = 11
        DF_PRIMA_DMT_FG_EMP.loc[
            DF_PRIMA_DMT_FG_EMP["SHIFT_TIMESHEET_IND"] == "Y", "BUCKET_ID"
        ] = 11

        # Place UK subcontractors in a different bucket
        DF_PRIMA_DMT_FG_EMP_INC.loc[
            DF_PRIMA_DMT_FG_EMP_INC["EMP_SERIAL"].str.startswith("C-"), "BUCKET_ID"
        ] = 12
        DF_PRIMA_DMT_FG_EMP.loc[
            DF_PRIMA_DMT_FG_EMP["EMP_SERIAL"].str.startswith("C-"), "BUCKET_ID"
        ] = 12

        DF_PRIMA_DMT_FG_EMP_INC.loc[
            DF_PRIMA_DMT_FG_EMP_INC["EMP_SERIAL"].str.startswith("Y"), "BUCKET_ID"
        ] = 12
        DF_PRIMA_DMT_FG_EMP.loc[
            DF_PRIMA_DMT_FG_EMP["EMP_SERIAL"].str.startswith("Y"), "BUCKET_ID"
        ] = 12

        self.log.log("Stage 380 - Capture Original Hours before any adjustments")

        DF_PRIMA_DMT_FG_EMP_INC_HOURS = pd.merge(
            DF_PRIMA_DMT_FG_EMP_INC,
            DF_PRIMA_GRP[
                [
                    "WS_DATE",
                    "WE_DATE",
                    "EMP_SERIAL",
                    "SATURDAY_HOURS",
                    "SUNDAY_HOURS",
                    "MONDAY_HOURS",
                    "TUESDAY_HOURS",
                    "WEDNESDAY_HOURS",
                    "THURSDAY_HOURS",
                    "FRIDAY_HOURS",
                    "SHIFT_TYPE",
                    "PRIMA_DAYS",
                ]
            ],
            how="inner",
            left_on="EMP_SERIAL",
            right_on="EMP_SERIAL",
            indicator=True,
        )
        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "ORIG_SATURDAY_HOURS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["SATURDAY_HOURS"]
        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "ORIG_SUNDAY_HOURS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["SUNDAY_HOURS"]
        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "ORIG_MONDAY_HOURS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["MONDAY_HOURS"]
        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "ORIG_TUESDAY_HOURS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["TUESDAY_HOURS"]
        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "ORIG_WEDNESDAY_HOURS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["WEDNESDAY_HOURS"]
        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "ORIG_THURSDAY_HOURS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["THURSDAY_HOURS"]
        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "ORIG_FRIDAY_HOURS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["FRIDAY_HOURS"]

        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "ORIG_PRIMA_DAYS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["PRIMA_DAYS"]

        self.log.log("Stage 390 - Capture Hours for fulle day / half day adjustments")

        writer = pd.ExcelWriter(
            r"{}/{}_tempoutput.xlsx".format(self.outputs, self.v_batch_id),
            engine="xlsxwriter",
        )
        workbook = writer.book
        DF_DMT.to_excel(writer, sheet_name="DF_DMT", index=False)
        DF_PRIMA_DMT_FG_EMP_INC_HOURS.to_excel(
            writer, sheet_name="DF_PRIMA_DMT_FG_EMP_INC_HOURS", index=False
        )

        writer.save()
        writer.close()

        try:
            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "SATURDAY_HOURS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.adjust_hours_as_per_loc(
                    x["PSL_RATE_CARD"],
                    x["EMP_COUNTRY"],
                    x["SATURDAY_HOURS"],
                    x["WS_DATE"],
                    0,
                ),
                axis=1,
            )
            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "SUNDAY_HOURS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.adjust_hours_as_per_loc(
                    x["PSL_RATE_CARD"],
                    x["EMP_COUNTRY"],
                    x["SUNDAY_HOURS"],
                    x["WS_DATE"],
                    1,
                ),
                axis=1,
            )
            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "MONDAY_HOURS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.adjust_hours_as_per_loc(
                    x["PSL_RATE_CARD"],
                    x["EMP_COUNTRY"],
                    x["MONDAY_HOURS"],
                    x["WS_DATE"],
                    2,
                ),
                axis=1,
            )
            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "TUESDAY_HOURS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.adjust_hours_as_per_loc(
                    x["PSL_RATE_CARD"],
                    x["EMP_COUNTRY"],
                    x["TUESDAY_HOURS"],
                    x["WS_DATE"],
                    3,
                ),
                axis=1,
            )
            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "WEDNESDAY_HOURS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.adjust_hours_as_per_loc(
                    x["PSL_RATE_CARD"],
                    x["EMP_COUNTRY"],
                    x["WEDNESDAY_HOURS"],
                    x["WS_DATE"],
                    4,
                ),
                axis=1,
            )
            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "THURSDAY_HOURS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.adjust_hours_as_per_loc(
                    x["PSL_RATE_CARD"],
                    x["EMP_COUNTRY"],
                    x["THURSDAY_HOURS"],
                    x["WS_DATE"],
                    5,
                ),
                axis=1,
            )
            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "FRIDAY_HOURS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.adjust_hours_as_per_loc(
                    x["PSL_RATE_CARD"],
                    x["EMP_COUNTRY"],
                    x["FRIDAY_HOURS"],
                    x["WS_DATE"],
                    6,
                ),
                axis=1,
            )

            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "PRIMA_DAYS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.cnt_working_days(x), axis=1
            )
        except Exception as e:
            self.log_notice(str(e))

        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "CAPADJ_SATURDAY_HOURS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["SATURDAY_HOURS"]
        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "CAPADJ_SUNDAY_HOURS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["SUNDAY_HOURS"]
        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "CAPADJ_MONDAY_HOURS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["MONDAY_HOURS"]
        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "CAPADJ_TUESDAY_HOURS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["TUESDAY_HOURS"]
        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "CAPADJ_WEDNESDAY_HOURS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["WEDNESDAY_HOURS"]
        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "CAPADJ_THURSDAY_HOURS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["THURSDAY_HOURS"]
        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "CAPADJ_FRIDAY_HOURS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["FRIDAY_HOURS"]
        DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            "CAPADJ_PRIMA_DAYS"
        ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS["PRIMA_DAYS"]

        self.log.log("Stage 400 - Capture Reduced hours for in month joiners / leavers")

        try:
            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "SATURDAY_HOURS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.remove_hours_for_in_month_joiners(
                    x["SATURDAY_HOURS"],
                    x["TS_START_DATE"],
                    x["TS_END_DATE"],
                    x["WS_DATE"],
                    0,
                    x["IN_MONTH_JOINER_OR_LEAVER"],
                ),
                axis=1,
            )
            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "SUNDAY_HOURS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.remove_hours_for_in_month_joiners(
                    x["SUNDAY_HOURS"],
                    x["TS_START_DATE"],
                    x["TS_END_DATE"],
                    x["WS_DATE"],
                    1,
                    x["IN_MONTH_JOINER_OR_LEAVER"],
                ),
                axis=1,
            )
            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "MONDAY_HOURS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.remove_hours_for_in_month_joiners(
                    x["MONDAY_HOURS"],
                    x["TS_START_DATE"],
                    x["TS_END_DATE"],
                    x["WS_DATE"],
                    2,
                    x["IN_MONTH_JOINER_OR_LEAVER"],
                ),
                axis=1,
            )
            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "TUESDAY_HOURS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.remove_hours_for_in_month_joiners(
                    x["TUESDAY_HOURS"],
                    x["TS_START_DATE"],
                    x["TS_END_DATE"],
                    x["WS_DATE"],
                    3,
                    x["IN_MONTH_JOINER_OR_LEAVER"],
                ),
                axis=1,
            )
            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "WEDNESDAY_HOURS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.remove_hours_for_in_month_joiners(
                    x["WEDNESDAY_HOURS"],
                    x["TS_START_DATE"],
                    x["TS_END_DATE"],
                    x["WS_DATE"],
                    4,
                    x["IN_MONTH_JOINER_OR_LEAVER"],
                ),
                axis=1,
            )
            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "THURSDAY_HOURS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.remove_hours_for_in_month_joiners(
                    x["THURSDAY_HOURS"],
                    x["TS_START_DATE"],
                    x["TS_END_DATE"],
                    x["WS_DATE"],
                    5,
                    x["IN_MONTH_JOINER_OR_LEAVER"],
                ),
                axis=1,
            )
            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "FRIDAY_HOURS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.remove_hours_for_in_month_joiners(
                    x["FRIDAY_HOURS"],
                    x["TS_START_DATE"],
                    x["TS_END_DATE"],
                    x["WS_DATE"],
                    6,
                    x["IN_MONTH_JOINER_OR_LEAVER"],
                ),
                axis=1,
            )

            DF_PRIMA_DMT_FG_EMP_INC_HOURS[
                "PRIMA_DAYS"
            ] = DF_PRIMA_DMT_FG_EMP_INC_HOURS.apply(
                lambda x: self.cnt_working_days(x), axis=1
            )
        except Exception as e:
            self.log_notice(str(e))

        self.log.log("Stage 410 - Capture Hours Summary")

        v_sql_text = """
        SELECT EMP_SERIAL,
            ORIG_PRIMA_HOURS,
            CAPPED_ADJUSTED_PRIMA_HOURS,
            FG_HOURS_CLOCKED,
            ORIG_PRIMA_HOURS - CAPPED_ADJUSTED_PRIMA_HOURS AS DIFF_ORIG_CAPPED,
            CAPPED_ADJUSTED_PRIMA_HOURS - FG_HOURS_CLOCKED AS DIFF_CAPPED_FG,
            ORIG_PRIMA_DAYS,
            CAPPED_ADJUSTED_PRIMA_DAYS,
            FG_DAYS_CLOCKED,
            ORIG_PRIMA_DAYS - CAPPED_ADJUSTED_PRIMA_DAYS AS DIFF_ORIG_CAPPED_DAYS,
            CAPPED_ADJUSTED_PRIMA_DAYS - FG_DAYS_CLOCKED AS DIFF_CAPPED_FG_DAYS
        FROM
        (
        SELECT EMP_SERIAL,
            SUM(coalesce(ORIG_SATURDAY_HOURS,0) + coalesce(ORIG_SUNDAY_HOURS,0) + coalesce(ORIG_MONDAY_HOURS,0) + coalesce(ORIG_TUESDAY_HOURS,0) + coalesce(ORIG_WEDNESDAY_HOURS,0) + coalesce(ORIG_THURSDAY_HOURS,0) + coalesce(ORIG_FRIDAY_HOURS,0)) AS ORIG_PRIMA_HOURS, 
            SUM(coalesce(CAPADJ_SATURDAY_HOURS,0) + coalesce(CAPADJ_SUNDAY_HOURS,0) + coalesce(CAPADJ_MONDAY_HOURS,0) + coalesce(CAPADJ_TUESDAY_HOURS,0) + coalesce(CAPADJ_WEDNESDAY_HOURS,0) + coalesce(CAPADJ_THURSDAY_HOURS,0) + coalesce(CAPADJ_FRIDAY_HOURS,0)) AS CAPPED_ADJUSTED_PRIMA_HOURS,
            SUM(coalesce(SATURDAY_HOURS,0) + coalesce(SUNDAY_HOURS,0) + coalesce(MONDAY_HOURS,0) + coalesce(TUESDAY_HOURS,0) + coalesce(WEDNESDAY_HOURS,0) + coalesce(THURSDAY_HOURS,0) + coalesce(FRIDAY_HOURS,0)) AS FG_HOURS_CLOCKED,
            SUM(ORIG_PRIMA_DAYS) AS ORIG_PRIMA_DAYS,
            SUM(CAPADJ_PRIMA_DAYS) AS CAPPED_ADJUSTED_PRIMA_DAYS,
            SUM(PRIMA_DAYS) AS FG_DAYS_CLOCKED
            FROM DF_PRIMA_DMT_FG_EMP_INC_HOURS
        GROUP BY  EMP_SERIAL
        ) A
        """
        DF_SUMMARY_HOURS = pandasql.sqldf(v_sql_text, locals())

        self.log.log("Stage 420 - Capture Hours Summary - changed hours only")

        v_sql_text = """
        SELECT EMP_SERIAL, BUCKET_ID,
            ORIG_PRIMA_HOURS,
            CAPPED_ADJUSTED_PRIMA_HOURS,
            FG_HOURS_CLOCKED,
            ORIG_PRIMA_HOURS - CAPPED_ADJUSTED_PRIMA_HOURS AS DIFF_ORIG_CAPPED,
            CAPPED_ADJUSTED_PRIMA_HOURS - FG_HOURS_CLOCKED AS DIFF_CAPPED_FG,
            ORIG_PRIMA_DAYS,
            CAPPED_ADJUSTED_PRIMA_DAYS,
            FG_DAYS_CLOCKED,
            ORIG_PRIMA_DAYS - CAPPED_ADJUSTED_PRIMA_DAYS AS DIFF_ORIG_CAPPED_DAYS,
            CAPPED_ADJUSTED_PRIMA_DAYS - FG_DAYS_CLOCKED AS DIFF_CAPPED_FG_DAYS
        FROM
        (
        SELECT EMP_SERIAL, BUCKET_ID,
            SUM(coalesce(ORIG_SATURDAY_HOURS,0) + coalesce(ORIG_SUNDAY_HOURS,0) + coalesce(ORIG_MONDAY_HOURS,0) + coalesce(ORIG_TUESDAY_HOURS,0) + coalesce(ORIG_WEDNESDAY_HOURS,0) + coalesce(ORIG_THURSDAY_HOURS,0) + coalesce(ORIG_FRIDAY_HOURS,0)) AS ORIG_PRIMA_HOURS, 
            SUM(coalesce(CAPADJ_SATURDAY_HOURS,0) + coalesce(CAPADJ_SUNDAY_HOURS,0) + coalesce(CAPADJ_MONDAY_HOURS,0) + coalesce(CAPADJ_TUESDAY_HOURS,0) + coalesce(CAPADJ_WEDNESDAY_HOURS,0) + coalesce(CAPADJ_THURSDAY_HOURS,0) + coalesce(CAPADJ_FRIDAY_HOURS,0)) AS CAPPED_ADJUSTED_PRIMA_HOURS,
            SUM(coalesce(SATURDAY_HOURS,0) + coalesce(SUNDAY_HOURS,0) + coalesce(MONDAY_HOURS,0) + coalesce(TUESDAY_HOURS,0) + coalesce(WEDNESDAY_HOURS,0) + coalesce(THURSDAY_HOURS,0) + coalesce(FRIDAY_HOURS,0)) AS FG_HOURS_CLOCKED,
            SUM(ORIG_PRIMA_DAYS) AS ORIG_PRIMA_DAYS,
            SUM(CAPADJ_PRIMA_DAYS) AS CAPPED_ADJUSTED_PRIMA_DAYS,
            SUM(PRIMA_DAYS) AS FG_DAYS_CLOCKED
            FROM DF_PRIMA_DMT_FG_EMP_INC_HOURS
        GROUP BY  EMP_SERIAL, BUCKET_ID
        ) A
        WHERE DIFF_CAPPED_FG <> 0
        """
        DF_SUMMARY_HOURS_CHG = pandasql.sqldf(v_sql_text, locals())

        DF_PRIMA_DMT_FG_EMP = pd.merge(
            DF_PRIMA_DMT_FG_EMP,
            DF_SUMMARY_HOURS,
            how="left",
            left_on="EMP_SERIAL",
            right_on="EMP_SERIAL",
        )

        self.log.log("Stage 430 - Create Final Data Frame")

        DF_FINAL = DF_PRIMA_DMT_FG_EMP_INC_HOURS[
            [
                "JOB_SEEKER_ID",
                "WS_DATE",
                "COST_CENTRE_CODE",
                "SUNDAY_HOURS",
                "MONDAY_HOURS",
                "TUESDAY_HOURS",
                "WEDNESDAY_HOURS",
                "THURSDAY_HOURS",
                "FRIDAY_HOURS",
                "SATURDAY_HOURS",
                "BUCKET_ID",
                "EMP_SERIAL",
                "BRID",
                "SHIFT_TYPE",
            ]
        ]
        DF_FINAL["SHIFT_TYPE"] = DF_FINAL["SHIFT_TYPE"].fillna(0)
        DF_FINAL["SHIFT_SORT"] = np.where(DF_FINAL["SHIFT_TYPE"] == 0, 0, 1)

        DF_FINAL.sort_values(
            ["JOB_SEEKER_ID", "SHIFT_SORT", "WS_DATE", "SHIFT_TYPE"],
            axis=0,
            ascending=True,
            inplace=True,
            na_position="last",
        )

        DF_FINAL.rename(
            index=str,
            columns={
                "JOB_SEEKER_ID": "Job_Seeker_Id",
                "WS_DATE": "Week_Start_Date",
                "COST_CENTRE_CODE": "Cost_Center_Code",
                "SUNDAY_HOURS": "Sun_Hrs",
                "MONDAY_HOURS": "Mon_Hrs",
                "TUESDAY_HOURS": "Tue_Hrs",
                "WEDNESDAY_HOURS": "Wed_Hrs",
                "THURSDAY_HOURS": "Thu_Hrs",
                "FRIDAY_HOURS": "Fri_Hrs",
                "SATURDAY_HOURS": "Sat_Hrs",
            },
            inplace=True,
        )

        self.log.log(
            "Stage 440 - Add additional columns as per template in Final Data Frame", 2
        )

        DF_FINAL["Last_Name"] = ""
        DF_FINAL["First_Name"] = ""
        DF_FINAL["Cost_Center_Name"] = ""
        DF_FINAL["GL_Account_Code"] = ""
        DF_FINAL["Segmented Object Detail"] = ""
        DF_FINAL["GL_Account_Name"] = ""

        writer = pd.ExcelWriter(
            r"{}/{}_tempoutput.xlsx".format(self.outputs, self.v_batch_id),
            engine="xlsxwriter",
        )
        workbook = writer.book
        DF_FINAL.to_excel(writer, sheet_name="DF_FINAL", index=False)
        DF_PRIMA_DMT_FG_EMP.to_excel(
            writer, sheet_name="DF_IPPF_DMT_FG_EMP", index=False
        )

        writer.save()
        writer.close()

        try:
            DF_FINAL["UOM"] = DF_FINAL.apply(
                lambda x: self.final_timesheet_apply_uom(x["SHIFT_TYPE"]), axis=1
            )
        except Exception as e:
            DF_FINAL["UOM"] = np.NaN
            self.log_notice(str(e))
        DF_FINAL["Date"] = self.v_month_start
        DF_FINAL["Task_Code"] = "Time Worked"
        DF_FINAL["Task_Name"] = "Time Worked"

        try:
            DF_FINAL["Rate_Category_Code"] = DF_FINAL.apply(
                lambda x: self.final_timesheet_apply_ratecat(x["SHIFT_TYPE"]), axis=1
            )
            DF_FINAL["Rate_Category_Name"] = DF_FINAL.apply(
                lambda x: self.final_timesheet_apply_ratecat(x["SHIFT_TYPE"]), axis=1
            )
        except Exception as e:
            DF_FINAL["Rate_Category_Code"] = np.NaN
            DF_FINAL["Rate_Category_Name"] = np.NaN
            self.log_notice(str(e))

        DF_FINAL["Shift Code"] = "Default"
        DF_FINAL["Shift Name"] = "Default"

        DF_FINAL = DF_FINAL[
            [
                "Job_Seeker_Id",
                "Last_Name",
                "First_Name",
                "Date",
                "Week_Start_Date",
                "Cost_Center_Code",
                "Cost_Center_Name",
                "Task_Code",
                "Task_Name",
                "GL_Account_Code",
                "Segmented Object Detail",
                "GL_Account_Name",
                "Rate_Category_Code",
                "Rate_Category_Name",
                "UOM",
                "Shift Code",
                "Shift Name",
                "Sun_Hrs",
                "Mon_Hrs",
                "Tue_Hrs",
                "Wed_Hrs",
                "Thu_Hrs",
                "Fri_Hrs",
                "Sat_Hrs",
                "EMP_SERIAL",
                "BRID",
                "BUCKET_ID",
            ]
        ]

        DF_FINAL.to_excel(
            f"{self.outputs}/{self.v_batch_id}_Bucket_All.xlsx",
            sheet_name="TIMESHEET",
            engine="xlsxwriter",
        )

        self.log.log("Stage 450 - Start Creating buckets", 2)

        i = 0

        while i < 14:

            DF_FINAL_TEMP = DF_FINAL[DF_FINAL["BUCKET_ID"] == i]

            if DF_FINAL_TEMP.shape[0] > 0:

                self.log.log("Creating file for Bucket {}".format(i), 2)

                DF_FINAL_TEMP = DF_FINAL_TEMP.drop(["BUCKET_ID"], axis=1)
                v_file_name = r"{}/{}_Bucket_{}_Timesheet.xlsx".format(
                    self.outputs, self.v_batch_id, i
                )
                writer = pd.ExcelWriter(v_file_name, engine="xlsxwriter")
                workbook = writer.book
                DF_FINAL_TEMP.to_excel(
                    writer, sheet_name="TIMESHEET", index=False, startrow=13
                )
                writer.save()
                writer.close()

                workbook = load_workbook(filename=v_file_name)
                sheet = workbook.active
                sheet["A1"] = "Type=Rate Schedule Based Time Sheet Upload"
                sheet["A2"] = "Transaction=False"
                sheet["A3"] = "Allow Custom Allocation=False"
                sheet["A4"] = "Approval Required=True"
                sheet["A5"] = "Send Notification?=True"
                sheet["A6"] = "Language=English (United States)"
                sheet["A7"] = "Number Format=#,##9.99 (Example: 1,234,567.99)"
                sheet["A8"] = "Date Format=YYYY-MM-DD"
                sheet["A9"] = "Submit=True"
                sheet["A10"] = "Supplier Review=False"
                sheet["A11"] = "Buyer=BCLY"
                sheet["A12"] = "Comments= {} Bucket {}".format(
                    self.v_comm, i
                )

                workbook.save(filename=v_file_name)
            i = i + 1

        DF_HOURS_SUMMARY = DF_PRIMA_DMT_FG_EMP[
            [
                "BUCKET_ID",
                "EMP_SERIAL",
                "EMP_LASTNAME",
                "FG_BRID",
                "TS_START_DATE",
                "TS_END_DATE",
                "ORIG_PRIMA_HOURS",
                "CAPPED_ADJUSTED_PRIMA_HOURS",
                "FG_HOURS_CLOCKED",
                "DIFF_ORIG_CAPPED",
                "DIFF_CAPPED_FG",
                "ORIG_PRIMA_DAYS",
                "CAPPED_ADJUSTED_PRIMA_DAYS",
                "FG_DAYS_CLOCKED",
                "DIFF_ORIG_CAPPED_DAYS",
                "DIFF_CAPPED_FG_DAYS",
                "SR_RESPONSE_DATE",
                "CAN_SR_START_DATE",
                "DATE_BRID_AVAILABLE",
                "DATE_RSA_TOKEN_AVAILABLE",
                "CAN_END_DATE",
                "EMP_COUNTRY",
                "SR_NO",
                "TC",
                "FINAL_EXCLUSION",
            ]
        ]

        DF_HOURS_SUMMARY = DF_HOURS_SUMMARY[
            DF_HOURS_SUMMARY["FINAL_EXCLUSION"] == "Timesheet Created"
        ]

        self.log.log("Stage 460 - Start OUTPUT file creation")

        # try:
        #     query = f"""SELECT * FROM {self.timesheet_exclusion_list}"""
        #     DF_TS_EXCL_PREV = pd.read_sql(query, self.conn)
        #     # DF_TS_EXCL_PREV = pd.read_excel(
        #     #     self.timesheet_exclusion_list, sheet_name="Sheet1"
        #     # )
        # except Exception:
        #     DF_TS_EXCL_PREV = pd.DataFrame()

        # DF_TS_EXCL = DF_PRIMA_DMT_FG_EMP

        cursor = self.conn.cursor()

        DF_PRIMA_DMT_FG_EMP = DF_PRIMA_DMT_FG_EMP.replace(np.NaN, "Missing")

        update_dt = datetime.strftime(datetime.now(), "%Y%m%d%H%M%S")

        for row in DF_PRIMA_DMT_FG_EMP.itertuples():
            if getattr(row, "FINAL_EXCLUSION") == "Timesheet Created":
                cursor.execute(
                    f"""
                UPDATE TIMESHEET_REPORT SET CREATED = 'Yes', UPDATED_DT = '{update_dt}',
                    ORIG_PRIMA_HOURS = {getattr(row, "ORIG_PRIMA_HOURS")}, CAPPED_ADJUSTED_PRIMA_HOURS = {getattr(row, 'CAPPED_ADJUSTED_PRIMA_HOURS')}, 
                    FG_HOURS_CLOCKED = {getattr(row, "FG_HOURS_CLOCKED")}, JOB_SEEKER_ID = '{getattr(row, 'JOB_SEEKER_ID')}', 
                    COST_CENTRE_CODE = '{getattr(row, "COST_CENTRE_CODE")}', SUBMISSION = '{self.v_comm}'
                WHERE EMP_SERIAL = '{getattr(row, 'EMP_SERIAL')}'
                    AND MONTH_START = '{self.v_ms_date}' AND MONTH_END = '{self.v_me_date}'
                    AND CREATED_DT = (SELECT MAX(CREATED_DT) FROM TIMESHEET_REPORT) AND TYPE = "Regular";
                """
                )

            else:
                cursor.execute(
                    f"""
                UPDATE TIMESHEET_REPORT SET CREATED = 'No', EXCLUSION = 'Yes', REASON = '{getattr(row, "FINAL_EXCLUSION")}',  UPDATED_DT = '{update_dt}',
                    JOB_SEEKER_ID = '{getattr(row, 'JOB_SEEKER_ID')}', COST_CENTRE_CODE = '{getattr(row, "COST_CENTRE_CODE")}'
                WHERE EMP_SERIAL = '{getattr(row, "EMP_SERIAL")}'
                    AND MONTH_START = '{self.v_ms_date}' AND MONTH_END = '{self.v_me_date}'
                    AND CREATED_DT = (SELECT MAX(CREATED_DT) FROM TIMESHEET_REPORT) AND TYPE = "Regular";
                """
                )

            r = self.conn.commit()

        cursor.close()

        writer = pd.ExcelWriter(
            r"{}/{}_output.xlsx".format(self.outputs, self.v_batch_id),
            engine="xlsxwriter",
        )
        workbook = writer.book
        DF_SUMMARY.to_excel(writer, sheet_name="DF_SUMMARY", index=False)

        DF_SUMMARY_HOURS_CHG.to_excel(
            writer, sheet_name="DF_HOURS_SUMMARY_CHG", index=False
        )
        DF_HOURS_SUMMARY.to_excel(writer, sheet_name="DF_HOURS_SUMMARY", index=False)
        DF_TS_HOLD = pd.read_sql(f"SELECT * FROM {self.timesheets_on_hold}", self.conn)
        DF_TS_HOLD.to_excel(writer, sheet_name="TIMESHEETS_ON_HOLD")
        DF_PRIMA_DMT_FG_EMP.to_excel(
            writer, sheet_name="DF_IPPF_DMT_FG_EMP", index=False
        )
        DF_CW.to_excel(writer, sheet_name="DF_CW", index=False)
        DF_CW_DUP.to_excel(writer, sheet_name="DF_CW_DUP", index=False)
        DF_CW_RES_STILLDUP.to_excel(
            writer, sheet_name="DF_CW_RES_STILLDUP", index=False
        )
        DF_CW_NEW.to_excel(writer, sheet_name="DF_CW_NEW", index=False)
        DF_CW_NOBRID.to_excel(writer, sheet_name="DF_CW_NOBRID", index=False)
        DF_PRIMA.to_excel(writer, sheet_name="DF_IPPF", index=False)
        DF_PRIMA_CH.to_excel(writer, sheet_name="DF_IPPF_CH", index=False)
        DF_PRIMA_GRP.to_excel(writer, sheet_name="DF_IPPF_GRP", index=False)
        DF_PRIMA_EMP_LIST.to_excel(writer, sheet_name="DF_IPPF_EMP_LIST", index=False)
        DF_PRIMA_DMT_EMP.to_excel(writer, sheet_name="DF_IPPF_DMT_EMP", index=False)
        DF_PRIMA_DMT_FG_EMP_INC_HOURS.to_excel(
            writer, sheet_name="INC_HOURS", index=False
        )
        DF_FINAL.to_excel(writer, sheet_name="DF_FINAL", index=False)
        DF_DMT.to_excel(writer, sheet_name="DF_DMT", index=False)
        DF_PRIMA_ONBR.to_excel(writer, sheet_name="DF_ONBR_GT90", index=False)

        self.helpers.add_validation_reports(writer, DF_FINAL)

        writer.save()
        writer.close()

        self.log.log("Stage 500 - Processing complete", 2)
        self.log.log("Done", 2)

    # run the program from any source
    def process_normal_hours(self):
        """
        Generate normal hours timesheet for the specified environment

        Two types of env are supported
        - `testing`     - run the program for the specified input directory containing test cases and produce the output on a specified directory
        - `production`  - run the program for the default directories
        """

        # -----------------------------------------------------------------------------
        print(f"Running on {self.v_env} environment")
        # -----------------------------------------------------------------------------

        # only show errors
        warnings.simplefilter(action="ignore")

        self.process_prima()

        self.v_log.close()


if __name__ == "__main__":
    ts = FG_Auto()
    ts.process_normal_hours()
